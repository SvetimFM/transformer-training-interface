<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformer Training UI</title>
    <link rel="stylesheet" href="/static/style.css">
    <link rel="stylesheet" href="/static/architecture.css">
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://d3js.org/d3.v7.min.js"></script>
</head>
<body>
    <div class="container">
        <h1>Transformer Training Interface</h1>
        
        <!-- Tab Navigation -->
        <div class="tab-navigation">
            <button class="tab-button active" data-tab="transformer">Transformer Training</button>
        </div>
        
        <!-- Tab Content -->
        <div class="tab-content">
            <!-- Standard Transformer Tab -->
            <div id="transformer-tab" class="tab-panel active">
                <!-- Scientific Explainer -->
                <div class="scientific-explainer">
                    <h2>Understanding Transformers</h2>
                    <p>
                        Transformers are neural architectures that revolutionized machine learning through <strong>self-attention mechanisms</strong>. 
                        Unlike RNNs, they process sequences in parallel, computing relationships between all positions simultaneously.
                    </p>
                    <div class="implementation-note">
                        <strong>üìù Implementation Note:</strong> This interface implements a GPT-style decoder-only transformer optimized for text generation, 
                        with several modern improvements over the original 2017 paper. Hover over configuration options to learn about these design choices.
                    </div>
                    <div class="explainer-grid">
                        <div class="explainer-item">
                            <h3>üß† Self-Attention</h3>
                            <p>Computes weighted relationships between all tokens in a sequence, allowing the model to focus on relevant context regardless of distance.</p>
                        </div>
                        <div class="explainer-item">
                            <h3>üìä Multi-Head Attention</h3>
                            <p>Parallel attention mechanisms that capture different types of relationships (syntactic, semantic, positional) simultaneously.</p>
                        </div>
                        <div class="explainer-item">
                            <h3>üîÑ Residual Connections</h3>
                            <p>Skip connections that help gradients flow and allow the model to learn identity mappings when needed.</p>
                        </div>
                        <div class="explainer-item">
                            <h3>üìê Layer Normalization</h3>
                            <p>Stabilizes training by normalizing activations across features, crucial for deep transformer architectures.</p>
                        </div>
                    </div>
                </div>
                
                <!-- Shakespeare Recipe -->
                <div class="recipe-section">
                    <h3>üé≠ Tiny Shakespeare Recipe</h3>
                    <p>The dataset contains ~1M characters of Shakespeare's works. Here's a proven recipe to get started:</p>
                    <div class="recipe-grid">
                        <div class="recipe-card">
                            <h4>Beginner (Fast Training)</h4>
                            <ul>
                                <li><strong>Layers:</strong> 2</li>
                                <li><strong>Embed Dim:</strong> 128</li>
                                <li><strong>Heads:</strong> 4</li>
                                <li><strong>Batch Size:</strong> 64</li>
                                <li><strong>Learning Rate:</strong> 3e-4</li>
                                <li><strong>Epochs:</strong> 5-10</li>
                            </ul>
                            <p class="recipe-note">~300K params, trains in minutes</p>
                        </div>
                        <div class="recipe-card">
                            <h4>Intermediate (Better Quality)</h4>
                            <ul>
                                <li><strong>Layers:</strong> 4</li>
                                <li><strong>Embed Dim:</strong> 256</li>
                                <li><strong>Heads:</strong> 8</li>
                                <li><strong>Batch Size:</strong> 32</li>
                                <li><strong>Learning Rate:</strong> 2e-4</li>
                                <li><strong>Epochs:</strong> 10-20</li>
                            </ul>
                            <p class="recipe-note">~2.5M params, good results</p>
                        </div>
                        <div class="recipe-card">
                            <h4>Advanced (Best Quality)</h4>
                            <ul>
                                <li><strong>Layers:</strong> 6</li>
                                <li><strong>Embed Dim:</strong> 384</li>
                                <li><strong>Heads:</strong> 6</li>
                                <li><strong>Batch Size:</strong> 16</li>
                                <li><strong>Learning Rate:</strong> 1e-4</li>
                                <li><strong>Epochs:</strong> 20-30</li>
                            </ul>
                            <p class="recipe-note">~10M params, authentic Shakespeare</p>
                        </div>
                    </div>
                    <div class="recipe-tips">
                        <h4>üí° Tips for Success</h4>
                        <ul>
                            <li>Enable <strong>Layer Norm</strong> and <strong>Residual Connections</strong> for stable training</li>
                            <li>Use <strong>Pre-norm</strong> position for better stability</li>
                            <li>Keep <strong>Torch Compile</strong> and <strong>Mixed Precision</strong> enabled for speed</li>
                            <li>Watch for loss < 1.5 for coherent text generation</li>
                            <li>Temperature 0.8-1.0 works well for Shakespeare-style text</li>
                        </ul>
                    </div>
                </div>
                
                <!-- Advanced Dataset & Tokenizer Section -->
                <div class="advanced-section">
                    <details>
                        <summary class="advanced-header">
                            <h3>üîß Advanced: Custom Datasets & Tokenizers</h3>
                            <span class="expand-icon">‚ñ∂</span>
                        </summary>
                        <div class="advanced-content">
                            <div class="dataset-panel">
                                <h4>üìÑ Dataset Options</h4>
                                <div class="dataset-options">
                                    <label class="radio-option">
                                        <input type="radio" name="dataset-type" value="shakespeare" checked>
                                        <span class="option-label">Tiny Shakespeare (Default)</span>
                                        <p class="option-desc">~1MB of Shakespeare's works, perfect for learning</p>
                                    </label>
                                    <label class="radio-option">
                                        <input type="radio" name="dataset-type" value="custom">
                                        <span class="option-label">Upload Custom Text</span>
                                        <p class="option-desc">Train on your own text (max 50MB)</p>
                                    </label>
                                    <label class="radio-option">
                                        <input type="radio" name="dataset-type" value="url">
                                        <span class="option-label">Load from URL</span>
                                        <p class="option-desc">Download text from a URL (cached locally)</p>
                                    </label>
                                </div>
                                
                                <div id="custom-dataset-input" class="dataset-input" style="display: none;">
                                    <input type="file" id="dataset-file" accept=".txt" class="file-input">
                                    <label for="dataset-file" class="file-label">Choose text file...</label>
                                </div>
                                
                                <div id="url-dataset-input" class="dataset-input" style="display: none;">
                                    <input type="url" id="dataset-url" placeholder="https://example.com/text.txt" class="url-input">
                                </div>
                            </div>
                            
                            <div class="tokenizer-panel">
                                <h4>üî§ Tokenizer Options</h4>
                                <div class="tokenizer-options">
                                    <label class="radio-option">
                                        <input type="radio" name="tokenizer-type" value="character" checked>
                                        <span class="option-label">Character-level</span>
                                        <p class="option-desc">Each character = 1 token. Simple, works with any text</p>
                                    </label>
                                    <label class="radio-option">
                                        <input type="radio" name="tokenizer-type" value="bpe">
                                        <span class="option-label">BPE (Byte-Pair Encoding)</span>
                                        <p class="option-desc">Subword tokens like GPT. More efficient, requires tiktoken</p>
                                    </label>
                                </div>
                                
                                <div id="bpe-options" class="tokenizer-config" style="display: none;">
                                    <label>BPE Model:</label>
                                    <select id="bpe-model" class="form-select">
                                        <option value="gpt2">GPT-2 (50k vocab)</option>
                                        <option value="gpt-3.5-turbo">GPT-3.5 (100k vocab)</option>
                                        <option value="custom">Train on dataset</option>
                                    </select>
                                </div>
                            </div>
                            
                            <div class="dataset-info">
                                <h4>üìä Dataset Information</h4>
                                <div class="info-grid">
                                    <div class="info-item">
                                        <span class="info-label">File Size:</span>
                                        <span class="info-value" id="dataset-size">1.1 MB</span>
                                    </div>
                                    <div class="info-item">
                                        <span class="info-label">Characters:</span>
                                        <span class="info-value" id="dataset-chars">1,115,394</span>
                                    </div>
                                    <div class="info-item">
                                        <span class="info-label">Vocabulary:</span>
                                        <span class="info-value" id="vocab-size">65 chars</span>
                                    </div>
                                    <div class="info-item">
                                        <span class="info-label">Est. Tokens:</span>
                                        <span class="info-value" id="token-count">1.1M</span>
                                    </div>
                                </div>
                                <div class="dataset-preview">
                                    <h5>Preview:</h5>
                                    <pre id="dataset-preview-text">First Citizen:
Before we proceed any further, hear me speak.

All:
Speak, speak.

First Citizen:
You are all resolved rather to die than to famish?</pre>
                                </div>
                            </div>
                            
                            <div class="advanced-tips">
                                <h4>üí° Tips for Custom Datasets</h4>
                                <ul>
                                    <li><strong>Character tokenizer:</strong> Best for structured text (code, poetry) or non-English languages</li>
                                    <li><strong>BPE tokenizer:</strong> Better for natural language, reduces sequence length by ~4x</li>
                                    <li><strong>Dataset size:</strong> Aim for at least 1MB of text for good results</li>
                                    <li><strong>Model size:</strong> Scale model parameters with dataset size (1M chars ‚âà 1M params)</li>
                                    <li><strong>Unicode:</strong> Character tokenizer handles any Unicode text automatically</li>
                                </ul>
                            </div>
                            
                            <button id="apply-dataset" class="btn btn-secondary">Apply Dataset & Tokenizer</button>
                        </div>
                    </details>
                </div>
                
                <div class="main-grid">
            <!-- Training Control Panel -->
            <div class="panel" id="training-panel">
                <h2 class="tooltip">Training Control
                    <span class="tooltiptext">Start, stop, and monitor training progress. Shows real-time metrics and GPU usage.</span>
                </h2>
                <div class="control-group">
                    <button id="start-training" class="btn btn-primary">Start Training</button>
                    <button id="stop-training" class="btn btn-secondary">Stop Training</button>
                </div>
                <div class="status-info">
                    <p>Status: <span id="training-status">Not Training</span></p>
                    <p>Epoch: <span id="current-epoch">0</span> / <span id="total-epochs">10</span></p>
                    <p>Step: <span id="current-step">0</span> / <span id="total-steps">5000</span></p>
                    <p>Loss: <span id="current-loss">0.000</span></p>
                    <p>Perplexity: <span id="current-perplexity">0.000</span></p>
                    <p>Learning Rate: <span id="current-lr">0.0003</span></p>
                    <p>Tokens/sec: <span id="tokens-per-second">0</span></p>
                    <p>GPU Memory: <span id="gpu-memory">0</span> MB</p>
                    <p>Gradient Norm: <span id="gradient-norm">-</span></p>
                </div>
                <div id="training-completion" class="completion-info" style="display: none;"></div>
            </div>
            
            <!-- Architecture Configuration -->
            <div class="panel" id="arch-panel">
                <h2 class="tooltip">Model Architecture
                    <span class="tooltiptext">Configure transformer layers, attention heads, and embedding dimensions. Note: This implements a GPT-style decoder-only architecture (autoregressive) rather than the original paper's encoder-decoder. Benefits: Simpler, better for text generation. Trade-off: No bidirectional context.</span>
                </h2>
                <div class="config-group">
                    <label class="switch-label tooltip">
                        <input type="checkbox" id="use-layer-norm" class="toggle-switch">
                        <span>Layer Normalization</span>
                        <span class="tooltiptext">Normalizes layer outputs to stabilize training. Essential for deep networks, prevents gradient explosion.</span>
                    </label>
                    <label class="switch-label tooltip">
                        <input type="checkbox" id="use-residual" class="toggle-switch">
                        <span>Residual Connections</span>
                        <span class="tooltiptext">Adds input directly to output (x + F(x)). Enables training of very deep networks by mitigating vanishing gradients.</span>
                    </label>
                    <div class="radio-group tooltip">
                        <label>Norm Position:</label>
                        <label><input type="radio" name="norm-position" value="pre" checked> Pre</label>
                        <label><input type="radio" name="norm-position" value="post"> Post</label>
                        <span class="tooltiptext">Pre-norm: LayerNorm before sublayers (modern best practice, more stable training). Post-norm: LayerNorm after (original paper design, may need careful warmup). We default to pre-norm as it's more forgiving for learners.</span>
                    </div>
                </div>
                
                <div class="slider-group">
                    <label class="tooltip">Layers: <span id="n-layers-value">1</span>
                        <span class="tooltiptext">Number of transformer blocks. More layers = more capacity but slower training. Start with 1-4 for experiments.</span>
                    </label>
                    <input type="range" id="n-layers" min="1" max="12" value="1" class="slider">
                    
                    <label for="n-embed" class="tooltip">Embed Dim: <span id="n-embed-value">256</span>
                        <span class="tooltiptext">Dimensionality of token embeddings and hidden states. Must be divisible by number of attention heads. Note: We use learned positional embeddings (like GPT) instead of sinusoidal (original paper). Benefits: Can learn task-specific patterns. Trade-off: Doesn't generalize to longer sequences.</span>
                    </label>
                    <input type="range" id="n-embed" min="64" max="1024" step="64" value="256" class="slider">
                    
                    <!-- Simple configuration for single layer -->
                    <div id="simple-config" style="display: block;">
                        <label class="tooltip">Heads: <span id="n-heads-value">8</span>
                            <span class="tooltiptext">Number of parallel attention heads. Each head learns different relationships (syntax, semantics, position). We offer two implementations: educational (separate heads, easier to understand) and standard (single projection, more efficient). Both produce similar results.</span>
                        </label>
                        <input type="range" id="n-heads" min="1" max="16" value="8" class="slider">
                        
                        <label class="tooltip">Hidden Multiplier: <span id="hidden-mult-value">4</span>x
                            <span class="tooltiptext">FFN hidden size = embed_dim √ó multiplier. Standard is 4x. Larger = more capacity but more compute.</span>
                        </label>
                        <input type="range" id="hidden-mult" min="1" max="8" value="4" class="slider">
                    </div>
                    
                    <!-- Per-layer configuration for multiple layers -->
                    <div id="per-layer-config" style="display: none;">
                        <h3>Per-Layer Configuration</h3>
                        <p style="font-size: 12px; color: #9ca3af; margin-bottom: 10px;">
                            Note: Transformer blocks use ReLU activation in their feedforward networks
                        </p>
                        <div id="layer-configs"></div>
                    </div>
                    
                    <div class="param-info">
                        <p>Total Parameters: <span id="total-params">0</span></p>
                    </div>
                </div>
                
                <h3>Output Layers Configuration</h3>
                <div class="config-group">
                    <label class="tooltip">Output Activation:
                        <span class="tooltiptext">Activation function for output projection layers. GELU is smooth and performs well in transformers.</span>
                    </label>
                    <select id="output-activation" class="form-select">
                        <option value="gelu">GELU</option>
                        <option value="relu">ReLU</option>
                        <option value="silu">SiLU (Swish)</option>
                        <option value="tanh">Tanh</option>
                    </select>
                    
                    <label class="tooltip">Hidden Layers: <span id="n-output-layers-value">0</span>
                        <span class="tooltiptext">Additional feedforward layers after transformer. 0 = direct projection to vocab. More layers can help with complex mappings.</span>
                    </label>
                    <input type="range" id="n-output-layers" min="0" max="5" value="0" class="slider">
                    
                    <div id="output-hidden-config" style="display: none;">
                        <label class="tooltip">Hidden Dimension: <span id="output-hidden-dim-value">512</span>
                            <span class="tooltiptext">Size of hidden layers in output projection. Typically between embed_dim and 2x embed_dim.</span>
                        </label>
                        <input type="range" id="output-hidden-dim" min="128" max="4096" step="128" value="512" class="slider">
                    </div>
                </div>
                
                <button id="apply-architecture" class="btn btn-secondary">Apply Changes</button>
            </div>
            
            <!-- Training Parameters -->
            <div class="panel" id="params-panel">
                <h2 class="tooltip">Training Parameters
                    <span class="tooltiptext">Set learning rate, batch size, and optimization settings. Note: We use AdamW optimizer (modern standard) instead of Adam from the original paper. AdamW decouples weight decay from gradient updates for better generalization.</span>
                </h2>
                <div class="input-group">
                    <label class="tooltip">Learning Rate:
                        <span class="tooltiptext">Step size for gradient descent. Start with 3e-4 for small models. Lower for larger models or if training is unstable.</span>
                    </label>
                    <input type="number" id="learning-rate" value="0.0003" step="0.0001" min="0.00001" max="0.1">
                    
                    <label class="tooltip">Batch Size:
                        <span class="tooltiptext">Number of sequences per gradient update. Larger = more stable but more memory. Power of 2 is optimal for GPUs.</span>
                    </label>
                    <input type="number" id="batch-size" value="64" min="1" max="256">
                    
                    <label class="tooltip">Epochs:
                        <span class="tooltiptext">Number of complete passes through the dataset. More epochs risk overfitting on small datasets.</span>
                    </label>
                    <input type="number" id="epochs" value="10" min="1" max="100">
                    
                    <label class="tooltip">Training Steps (optional):
                        <span class="tooltiptext">Total gradient updates. If set, overrides epochs. Useful for precise training budgets.</span>
                    </label>
                    <input type="number" id="train-steps" placeholder="Auto-calculate from epochs" min="100" max="100000">
                    
                    <label class="tooltip">LR Scheduler:
                        <span class="tooltiptext">How learning rate changes during training. Warmup+Cosine is modern best practice (smoother than linear decay in original paper). Benefits: Better final performance. Trade-off: Slightly more complex to tune.</span>
                    </label>
                    <select id="scheduler-type">
                        <option value="warmup_cosine">Warmup + Cosine</option>
                        <option value="warmup_linear">Warmup + Linear</option>
                        <option value="warmup_constant">Warmup + Constant</option>
                        <option value="onecycle">One Cycle</option>
                    </select>
                    
                    <label class="tooltip">Warmup Ratio: <span id="warmup-ratio-value">5</span>%
                        <span class="tooltiptext">Percentage of training with linearly increasing LR. Helps stabilize early training. 5-10% is typical.</span>
                    </label>
                    <input type="range" id="warmup-ratio" min="0" max="20" step="1" value="5" class="slider">
                    
                    <label class="tooltip">Min LR Ratio: <span id="min-lr-ratio-value">10</span>%
                        <span class="tooltiptext">Final LR as percentage of initial. Lower values = more aggressive decay. 10% is a good default.</span>
                    </label>
                    <input type="range" id="min-lr-ratio" min="0" max="50" step="5" value="10" class="slider">
                </div>
                
                <h3>Optimization Settings</h3>
                <div class="config-group">
                    <label class="switch-label tooltip">
                        <input type="checkbox" id="compile-model" class="toggle-switch" checked>
                        <span>Torch Compile</span>
                        <span class="tooltiptext">PyTorch 2.0+ feature: JIT compilation for ~2x speedup. Not in original paper but essential for modern training. Trade-off: Longer startup time, harder to debug.</span>
                    </label>
                    <label class="switch-label tooltip">
                        <input type="checkbox" id="use-amp" class="toggle-switch" checked>
                        <span>Mixed Precision (FP16)</span>
                        <span class="tooltiptext">Use 16-bit floats for faster training and less memory. Modern optimization not in original paper. Benefits: 2x memory savings, faster training. Trade-off: Rare numerical instabilities.</span>
                    </label>
                    
                    <label class="tooltip">Gradient Accumulation Steps:
                        <span class="tooltiptext">Accumulate gradients over N batches before updating. Simulates larger batch size without more memory.</span>
                    </label>
                    <input type="number" id="gradient-accumulation-steps" value="1" min="1" max="32">
                    
                    <label class="tooltip">Gradient Clip Norm:
                        <span class="tooltiptext">Maximum gradient norm. Prevents exploding gradients. 1.0 is standard for transformers.</span>
                    </label>
                    <input type="number" id="gradient-clip-norm" value="1.0" step="0.1" min="0.1" max="10">
                </div>
                
                <button id="update-params" class="btn btn-secondary">Update Parameters</button>
                <button id="preview-schedule" class="btn btn-secondary">Preview Schedule</button>
            </div>
            
            <!-- Loss Chart -->
            <div class="panel chart-panel" id="loss-chart-panel">
                <h2 class="tooltip">Training Metrics
                    <span class="tooltiptext">Real-time visualization of loss and validation metrics. Lower loss = better predictions.</span>
                </h2>
                <div style="height: 300px; position: relative;">
                    <canvas id="loss-chart"></canvas>
                </div>
            </div>
            
            <!-- Learning Rate Schedule -->
            <div class="panel" id="lr-schedule-panel">
                <h2 class="tooltip">Learning Rate Schedule
                    <span class="tooltiptext">Shows how learning rate changes during training. Warmup prevents instability, decay improves convergence.</span>
                </h2>
                <div style="height: 250px; position: relative;">
                    <canvas id="lr-schedule-chart"></canvas>
                </div>
                <div class="lr-schedule-info">
                    <p>Current Phase: <span id="lr-phase">Not Started</span></p>
                    <p>Schedule Type: <span id="lr-schedule-type">Warmup + Cosine</span></p>
                </div>
            </div>
            
            <!-- Architecture Visualization -->
            <div class="panel chart-panel" id="architecture-panel">
                <h2>Model Architecture (Experimental)</h2>
                <div class="architecture-controls">
                    <button id="reset-zoom">Reset View</button>
                    <button id="toggle-animations">Toggle Animations</button>
                    <label class="switch-label">
                        <input type="checkbox" id="detail-level" class="toggle-switch">
                        <span>Detailed View</span>
                    </label>
                    <label class="switch-label">
                        <input type="checkbox" id="visualization-mode" class="toggle-switch">
                        <span>Visualization Mode</span>
                    </label>
                    <div id="visualization-speed-control" style="display: none;">
                        <label>Speed: <span id="visualization-speed-value">1</span>%</label>
                        <input type="range" id="visualization-speed" min="1" max="100" value="1" class="slider">
                    </div>
                </div>
                <div id="architecture-container"></div>
                <div class="component-details" id="component-details">
                    <h4 id="component-name"></h4>
                    <div id="component-params"></div>
                </div>
                <div class="architecture-legend">
                    <div class="legend-item">
                        <div class="legend-color" style="background-color: #9333ea;"></div>
                        <span>Embedding</span>
                    </div>
                    <div class="legend-item">
                        <div class="legend-color" style="background-color: #3b82f6;"></div>
                        <span>Linear</span>
                    </div>
                    <div class="legend-item">
                        <div class="legend-color" style="background-color: #10b981;"></div>
                        <span>Attention</span>
                    </div>
                    <div class="legend-item">
                        <div class="legend-color" style="background-color: #f59e0b;"></div>
                        <span>Normalization</span>
                    </div>
                    <div class="legend-item">
                        <div class="legend-color" style="background-color: #ef4444;"></div>
                        <span>Dropout</span>
                    </div>
                </div>
            </div>
            
            <!-- Text Generation -->
            <div class="panel" id="generation-panel">
                <h2 class="tooltip">Text Generation
                    <span class="tooltiptext">Test your trained model by generating text. Temperature controls randomness (lower = more predictable).</span>
                </h2>
                <div class="input-group">
                    <label>Prompt:</label>
                    <input type="text" id="prompt-input" placeholder="Enter prompt text...">
                    
                    <label>Max Tokens: <span id="max-tokens-value">100</span></label>
                    <input type="range" id="max-tokens" min="10" max="500" value="100" class="slider">
                    
                    <label>Temperature: <span id="temperature-value">1.0</span></label>
                    <input type="range" id="temperature" min="0.1" max="2.0" step="0.1" value="1.0" class="slider">
                </div>
                <button id="generate-text" class="btn btn-primary">Generate</button>
                <div class="output-box" id="generated-text"></div>
                
                <div style="margin-top: 20px;">
                    <a href="/static/attention.html" class="btn btn-secondary">View Attention Patterns ‚Üí</a>
                </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <script src="/static/architecture.js?v=1.0.0"></script>
    <script src="/static/main.js?v=1.0.0"></script>
</body>
</html>
            <div id="pcn-experiments-tab" class="tab-panel">
                <div class="main-grid">
                    <!-- Data Leakage Demonstration Panel -->
                    <div class="panel" id="pcn-leakage-panel">
                        <h2>PCN Suspicious Accuracy Claims</h2>
                        <div class="pcn-intro">
                            <p>This experiment investigates the suspicious claims made in a recent Predictive Coding Network (PCN) paper.</p>
                            <p><strong>Paper:</strong> <a href="https://arxiv.org/pdf/2506.06332" target="_blank">Introduction to Predictive Coding Networks for Machine Learning</a></p>
                            <p><strong>Claim:</strong> The paper reports an extraordinary 99.92% accuracy on CIFAR-10 with only 4 epochs of training (4 minutes), which appears too good to be true for this challenging dataset.</p>
                        </div>
                        <div class="control-group">
                            <button id="start-pcn-experiment" class="btn btn-primary">Run Comparison</button>
                            <button id="stop-pcn-experiment" class="btn btn-secondary">Stop Experiment</button>
                        </div>
                        <div class="code-comparison">
                            <h3>Critical Implementation Issue</h3>
                            <div class="code-blocks">
                                <div class="code-block">
                                    <h4>‚ùå Problematic Implementation</h4>
                                    <pre><code># test_generative uses labels during inference!
def test_generative(pcn, x, y):
    # This is cheating - y (labels) used in inference
    pcn.infer(x, y, n_iter=50)
    predictions = pcn.mus[-1]  # Get final layer activations
    return accuracy(predictions, y)

# Training reports 99.92% accuracy
# but only because labels leak into test</code></pre>
                                </div>
                                <div class="code-block">
                                    <h4>‚úÖ Correct Implementation</h4>
                                    <pre><code># Proper inference without labels
def test_discriminative(pcn, x):
    # No labels during inference
    pcn.infer(x, n_iter=50)
    predictions = pcn.mus[-1]
    return predictions

# Real accuracy: ~45% after 4 epochs
# (which is expected for CIFAR-10)</code></pre>
                                </div>
                            </div>
                        </div>
                        <div class="comparison-metrics">
                            <div class="metric-box">
                                <h3>PCN (Paper's Claims)</h3>
                                <p>Accuracy: <span id="pcn-accuracy-leaked">0.00%</span></p>
                                <p>Training Time: <span id="pcn-train-time">4 min</span></p>
                                <p>Epochs: <span id="pcn-epochs">4</span></p>
                            </div>
                            <div class="metric-box">
                                <h3>Realistic PCN Performance</h3>
                                <p>Accuracy: <span id="pcn-accuracy-clean">0.00%</span></p>
                                <p>Expected Time: <span id="pcn-expected-time">2+ hours</span></p>
                                <p>Expected Epochs: <span id="pcn-expected-epochs">100+</span></p>
                            </div>
                        </div>
                        <div style="height: 300px; position: relative;">
                            <canvas id="pcn-comparison-chart"></canvas>
                        </div>
                        
                        <!-- Experiment Output Data -->
                        <div class="experiment-outputs">
                            <h3>Experiment Results</h3>
                            <div class="output-grid">
                                <div class="output-box">
                                    <h4>With Label Leakage (Problematic)</h4>
                                    <div class="output-content" id="pcn-output-leaked">
                                        <p class="output-placeholder">Run experiment to see results...</p>
                                    </div>
                                </div>
                                <div class="output-box">
                                    <h4>Without Label Leakage (Correct)</h4>
                                    <div class="output-content" id="pcn-output-clean">
                                        <p class="output-placeholder">Run experiment to see results...</p>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <!-- PCN Exploration Analysis Panel -->
                    <div class="panel" id="pcn-exploration-panel">
                        <h2>PCN Exploration Analysis</h2>
                        <div class="exploration-controls">
                            <label>Number of Samples: <span id="pcn-samples-value">10</span></label>
                            <input type="range" id="pcn-samples" min="1" max="50" value="10" class="slider">
                            
                            <label>Refinement Steps: <span id="pcn-refine-value">5</span></label>
                            <input type="range" id="pcn-refine-steps" min="1" max="20" value="5" class="slider">
                            
                            <label>Noise Scale: <span id="pcn-noise-value">0.1</span></label>
                            <input type="range" id="pcn-noise-scale" min="0.01" max="1.0" step="0.01" value="0.1" class="slider">
                        </div>
                        <div class="exploration-charts-vertical">
                            <div class="chart-container">
                                <h3>Energy Distribution</h3>
                                <canvas id="pcn-energy-chart"></canvas>
                            </div>
                            <div class="chart-container">
                                <h3>Diversity Scores</h3>
                                <canvas id="pcn-diversity-chart"></canvas>
                            </div>
                        </div>
                    </div>
                    
                    <!-- PCN Results Panel -->
                    <div class="panel" id="pcn-results-panel">
                        <h2>Experiment Results</h2>
                        <div class="results-summary">
                            <h3>Key Findings</h3>
                            <ul id="pcn-findings-list">
                                <li>Run experiments to see results...</li>
                            </ul>
                        </div>
                        <div class="scientific-implications">
                            <h3>Scientific Implications</h3>
                            <p id="pcn-implications">The claimed 99.92% CIFAR-10 accuracy with minimal training suggests potential issues: 
                            data leakage, test set memorization, or evaluation errors. Such results contradict decades of deep learning research 
                            showing CIFAR-10 requires substantial training for high accuracy.</p>
                        </div>
                    </div>
                </div>
            </div>
            
            <!-- Hybrid Models Tab -->
            <div id="hybrid-models-tab" class="tab-panel">
                <div class="main-grid">
                    <!-- Architecture Selector Panel -->
                    <div class="panel" id="hybrid-arch-panel">
                        <h2>Hybrid Architecture Selection</h2>
                        <div class="arch-selector">
                            <label>Select Architecture:</label>
                            <select id="hybrid-architecture" class="form-select">
                                <option value="pcn-ff">PCN-FF: PCN replaces Feedforward</option>
                                <option value="alternating">Alternating: Attention ‚Üî PCN layers</option>
                                <option value="hierarchical">Hierarchical: PCN features ‚Üí Transformer</option>
                                <option value="dual-stream">Dual-Stream: Parallel PCN + Transformer</option>
                                <option value="pcn-positional">PCN-Positional: Adaptive positional encoding</option>
                            </select>
                            <div class="arch-description" id="arch-description">
                                <p>Select an architecture to see its description and diagram.</p>
                            </div>
                        </div>
                        <div id="hybrid-arch-diagram"></div>
                    </div>
                    
                    <!-- Hybrid Training Configuration -->
                    <div class="panel" id="hybrid-config-panel">
                        <h2>Hybrid Model Configuration</h2>
                        <div class="config-group">
                            <h3>PCN-Specific Parameters</h3>
                            <label>PCN Learning Rate: <span id="pcn-lr-value">0.01</span></label>
                            <input type="range" id="pcn-learning-rate" min="0.001" max="0.1" step="0.001" value="0.01" class="slider">
                            
                            <label>Inference Steps: <span id="pcn-inference-value">10</span></label>
                            <input type="range" id="pcn-inference-steps" min="1" max="50" value="10" class="slider">
                            
                            <label>Energy Threshold: <span id="pcn-threshold-value">0.01</span></label>
                            <input type="range" id="pcn-energy-threshold" min="0.001" max="0.1" step="0.001" value="0.01" class="slider">
                            
                            <label class="switch-label">
                                <input type="checkbox" id="use-pcn-exploration" class="toggle-switch" checked>
                                <span>Enable PCN Exploration</span>
                            </label>
                        </div>
                        <button id="start-hybrid-training" class="btn btn-primary">Start Hybrid Training</button>
                        <button id="stop-hybrid-training" class="btn btn-secondary">Stop Training</button>
                    </div>
                    
                    <!-- Performance Comparison Panel -->
                    <div class="panel chart-panel" id="hybrid-comparison-panel">
                        <h2>Performance Comparison</h2>
                        <div class="comparison-controls">
                            <label class="switch-label">
                                <input type="checkbox" id="show-baseline" class="toggle-switch" checked>
                                <span>Show Standard Transformer Baseline</span>
                            </label>
                            <label class="switch-label">
                                <input type="checkbox" id="show-efficiency" class="toggle-switch">
                                <span>Show Efficiency Metrics</span>
                            </label>
                        </div>
                        <div style="height: 400px; position: relative;">
                            <canvas id="hybrid-performance-chart"></canvas>
                        </div>
                        <div class="performance-metrics">
                            <div class="metric-box">
                                <h3>Hybrid Model</h3>
                                <p>Loss: <span id="hybrid-loss">-</span></p>
                                <p>Perplexity: <span id="hybrid-perplexity">-</span></p>
                            </div>
                            <div class="metric-box">
                                <h3>Standard Transformer</h3>
                                <p>Loss: <span id="baseline-loss">-</span></p>
                                <p>Perplexity: <span id="baseline-perplexity">-</span></p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <!-- Multi-LoRA Tab -->
            <div id="multi-lora-tab" class="tab-panel">
                <div class="multi-lora-grid">
                    <!-- Multi-LoRA Configuration -->
                    <div class="panel" id="multi-lora-config-panel">
                        <h2>Multi-LoRA Configuration</h2>
                        <div class="config-section">
                            <h3>LoRA Settings</h3>
                            <div class="slider-group">
                                <label>Number of LoRAs: <span id="num-loras-value">4</span></label>
                                <input type="range" id="num-loras" min="2" max="8" value="4" class="slider">
                                
                                <label>LoRA Rank: <span id="lora-rank-value">8</span></label>
                                <input type="range" id="lora-rank" min="1" max="32" value="8" class="slider">
                                
                                <label>LoRA Alpha: <span id="lora-alpha-value">16</span></label>
                                <input type="range" id="lora-alpha" min="1" max="64" value="16" class="slider">
                            </div>
                        </div>
                        
                        <div class="config-section">
                            <h3>Selection Mode</h3>
                            <div class="radio-group">
                                <label>
                                    <input type="radio" name="lora-selection-mode" value="hard" checked>
                                    Hard Selection (Min Error)
                                </label>
                                <label>
                                    <input type="radio" name="lora-selection-mode" value="soft">
                                    Soft Selection (Gated)
                                </label>
                                <label>
                                    <input type="radio" name="lora-selection-mode" value="all">
                                    All LoRAs (Analysis Only)
                                </label>
                            </div>
                            
                            <div id="soft-selection-options" style="display: none;">
                                <label>Temperature: <span id="temperature-value">1.0</span></label>
                                <input type="range" id="selection-temperature" min="0.1" max="5.0" step="0.1" value="1.0" class="slider">
                            </div>
                        </div>
                        
                        <div class="config-section">
                            <h3>Target Modules</h3>
                            <div class="checkbox-group">
                                <label class="switch-label">
                                    <input type="checkbox" class="toggle-switch" value="query" checked>
                                    <span>Query Projections</span>
                                </label>
                                <label class="switch-label">
                                    <input type="checkbox" class="toggle-switch" value="value" checked>
                                    <span>Value Projections</span>
                                </label>
                                <label class="switch-label">
                                    <input type="checkbox" class="toggle-switch" value="key">
                                    <span>Key Projections</span>
                                </label>
                                <label class="switch-label">
                                    <input type="checkbox" class="toggle-switch" value="output_proj">
                                    <span>Output Projections</span>
                                </label>
                            </div>
                        </div>
                        
                        <button id="start-multi-lora-training" class="btn btn-primary">Start Multi-LoRA Training</button>
                        <button id="stop-multi-lora-training" class="btn btn-secondary">Stop Training</button>
                    </div>
                    
                    <!-- LoRA Selection Distribution -->
                    <div class="panel chart-panel" id="lora-selection-panel">
                        <h2>LoRA Selection Statistics</h2>
                        <div class="metric-highlight">
                            <p>Specialization Score: <span id="specialization-score">0.000</span></p>
                            <small>Lower scores indicate more specialized LoRAs</small>
                        </div>
                        <div style="height: 300px; position: relative;">
                            <canvas id="lora-selection-chart"></canvas>
                        </div>
                    </div>
                    
                    <!-- LoRA Performance Comparison -->
                    <div class="panel chart-panel" id="lora-performance-panel">
                        <h2>Per-LoRA Performance</h2>
                        <div style="height: 300px; position: relative;">
                            <canvas id="lora-performance-chart"></canvas>
                        </div>
                    </div>
                    
                    <!-- Token Selection Heatmap -->
                    <div class="panel chart-panel" id="token-heatmap-panel" style="grid-column: 1 / -1;">
                        <h2>Token-wise LoRA Selection Pattern</h2>
                        <div class="heatmap-controls">
                            <label>
                                Sequence Window: 
                                <select id="heatmap-window">
                                    <option value="last">Last Batch</option>
                                    <option value="avg">Running Average</option>
                                    <option value="specific">Specific Example</option>
                                </select>
                            </label>
                            <label class="switch-label">
                                <input type="checkbox" id="show-token-text" class="toggle-switch">
                                <span>Show Token Text</span>
                            </label>
                        </div>
                        <div id="token-selection-heatmap" style="height: 250px; overflow-x: auto;"></div>
                    </div>
                    
                    <!-- Multi-LoRA Analysis -->
                    <div class="panel" id="multi-lora-analysis-panel" style="grid-column: 1 / -1;">
                        <h2>LoRA Specialization Analysis</h2>
                        <div class="analysis-grid">
                            <div class="analysis-box">
                                <h3>Token Type Preferences</h3>
                                <div id="token-type-analysis">
                                    <p class="placeholder">Run training to see token type analysis...</p>
                                </div>
                            </div>
                            <div class="analysis-box">
                                <h3>Position-based Selection</h3>
                                <div id="position-analysis">
                                    <p class="placeholder">Run training to see position-based patterns...</p>
                                </div>
                            </div>
                            <div class="analysis-box">
                                <h3>Linguistic Pattern Clustering</h3>
                                <div id="pattern-clustering">
                                    <p class="placeholder">Run training to see pattern clusters...</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <script src="/static/architecture.js?v=1.0.0"></script>
    <script src="/static/main.js?v=1.0.0"></script>
    <script src="/static/pcn_visualizations.js?v=1.0.0"></script>
    <script src="/static/multi_lora_viz.js?v=1.0.0"></script>
</body>
</html>