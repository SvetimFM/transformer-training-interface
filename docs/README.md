# Documentation

Welcome to the Transformer Training UI documentation!

## üìö Tutorials and Guides

- **[Transformer Tutorial](TRANSFORMER_TUTORIAL.md)** - Comprehensive guide for understanding and training transformers
- **[Attention Visualization Guide](../ATTENTION_VISUALIZATION.md)** - Learn how attention patterns work
- **[LoRA Fine-tuning Guide](../LORA_README.md)** - Efficient model adaptation techniques

## üî¨ Research Documentation

- **[PCN Analysis Reports](../analysis/reports/)** - Research findings on Predictive Coding Networks
- **[Quick Start Guide](../analysis/QUICK_START.md)** - Fast track to running experiments

## üõ†Ô∏è Technical Documentation

- **[API Documentation](API_DOCUMENTATION.md)** - *(Coming Soon)* - Complete API reference
- **[Contributing Guide](../CONTRIBUTING.md)** - How to contribute to the project
- **[Project Summary](../PROJECT_SUMMARY.md)** - High-level project overview

## üß™ Experimental Features

Our experimental features are cutting-edge implementations that are still being refined:

- **Hybrid Models** - PCN-Transformer architectures combining the best of both worlds
- **Multi-LoRA** - Train multiple LoRA adapters simultaneously
- **Architecture Visualization** - Interactive model structure visualization

These features are functional but may have limitations or undergo significant changes.

## üìñ Additional Resources

- [Project Repository](https://github.com/SvetimFM/transformer-pcn-ui)
- [Issues and Support](https://github.com/SvetimFM/transformer-pcn-ui/issues)
- [Original Transformer Paper](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)